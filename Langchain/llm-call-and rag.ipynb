{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ[\"GOOGLE_API_KEY\"] = os.getenv(\"GOOGLE_API_KEY\")\n",
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "\n",
    "# Langsmith Tracking\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = \"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\"\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"First\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client=<openai.resources.chat.completions.Completions object at 0x0000021E22F700E0> async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x0000021E24B87CB0> root_client=<openai.OpenAI object at 0x0000021E24ADE4E0> root_async_client=<openai.AsyncOpenAI object at 0x0000021E24B878C0> model_name='o3-mini' model_kwargs={} openai_api_key=SecretStr('**********')\n"
     ]
    }
   ],
   "source": [
    "# For OpenAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "openllm = ChatOpenAI(model=\"o3-mini\")\n",
    "print(openllm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For groq\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "llm = ChatGroq(model=\"mixtral-8x7b-32768\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Imagine a regular computer bit like a light switch: it can be either ON (1) or OFF (0).  A quantum bit, or qubit, is like a special light switch that can be both ON and OFF *at the same time*.  This \"both at once\" state is called superposition.\\n\\nThink of it like flipping a coin: while it\\'s spinning in the air, it\\'s both heads and tails *potentially*.  Only when it lands does it become definitely one or the other.  Qubits are similar – they exist in a combination of states until they are measured.\\n\\nAnother key idea is entanglement.  This is where two or more qubits become linked, even if they\\'re far apart.  If you measure one entangled qubit, you instantly know the state of the other, like having two magically connected coins that always land on opposite sides.\\n\\nBecause qubits can be in multiple states at once, and because they can be entangled, quantum computers can explore many possibilities simultaneously. This allows them to tackle certain types of problems much faster than regular computers, like:\\n\\n* **Drug discovery:** Simulating molecules to design new medicines.\\n* **Materials science:** Creating new materials with specific properties.\\n* **Financial modeling:** Predicting market trends and managing risk.\\n* **Cryptography:** Breaking current encryption methods and developing new ones.\\n\\nIt\\'s important to note that quantum computers are not meant to replace regular computers entirely. They are specialized tools for specific tasks.  They are still in early stages of development, but they hold incredible potential to revolutionize various fields.' additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []} id='run-dd290dd7-f7bb-4436-ba3e-43d0d4bea956-0' usage_metadata={'input_tokens': 8, 'output_tokens': 327, 'total_tokens': 335, 'input_token_details': {'cache_read': 0}}\n"
     ]
    }
   ],
   "source": [
    "# For Gemini\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\")\n",
    "response = llm.invoke(\"Explain Quantum Computing in simple terms.\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"Agentic AI refers to\\nto artificial intelligence systems that are designed to act as autonomous agents, capable of making decisions and taking actions independently in order to achieve specific goals or objectives. These agents are designed to be proactive and responsive, able to perceive their environment, reason about it, and take appropriate actions to achieve their objectives.\\n\\nAgentic AI systems can be found in a variety of applications, including robotics, autonomous vehicles, and intelligent personal assistants. They are often designed to operate in complex and dynamic environments, where they must be able to adapt to changing conditions and make decisions in real-time.\\n\\nOne of the key challenges in developing agentic AI systems is ensuring that they are able to make decisions that are aligned with human values and ethical principles. This requires careful design and testing to ensure that the system's objectives and decision-making processes are transparent, understandable, and accountable.\\n\\nOverall, agentic AI has the potential to enable a wide range of new and innovative applications, from improving the efficiency and safety of industrial processes to enhancing the quality of life for individuals. However, it also raises important ethical and social questions that must be carefully considered as these systems are developed and deployed.\" additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 254, 'prompt_tokens': 13, 'total_tokens': 267, 'completion_time': 0.39148299, 'prompt_time': 0.002275374, 'queue_time': 0.03251411, 'total_time': 0.393758364}, 'model_name': 'mixtral-8x7b-32768', 'system_fingerprint': 'fp_c5f20b5bb1', 'finish_reason': 'stop', 'logprobs': None} id='run-df0319d2-5fc3-4170-9db4-7b427539eed3-0' usage_metadata={'input_tokens': 13, 'output_tokens': 254, 'total_tokens': 267}\n"
     ]
    }
   ],
   "source": [
    "response = llm.invoke(\"What is Agentic AI?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are an expert Data Scientist and Gen AI Engineer. Provide me answers based on the asked question '), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt=ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\"You are an expert Data Scientist and Gen AI Engineer. Provide me answers based on the asked question \"),\n",
    "        (\"user\",\"{input}\")\n",
    "    ]\n",
    ")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='LangChain is a powerful and flexible framework specifically designed for developing applications powered by language models. It goes beyond simply calling an LLM\\'s API by providing a structured approach to managing and optimizing interactions with these models. Think of it as a sophisticated toolbox filled with components and utilities that streamline the process of building complex, LLM-driven applications.\\n\\nHere\\'s a breakdown of key aspects of LangChain:\\n\\n**Core Concepts:**\\n\\n* **LLMs as a core component, but not the entire application:** LangChain recognizes that while LLMs are powerful, they are most effective when integrated with other sources of computation or knowledge.  This framework facilitates the connection of LLMs to other data sources and allows them to interact with their environment.\\n* **Chains:**  The central concept in LangChain is the \"chain.\"  Chains are sequences of calls to LLMs or other utilities. They allow for complex workflows and enable developers to orchestrate sophisticated interactions.  A simple chain might involve prompting an LLM, parsing its output, and then using that output to query a database. More complex chains can be constructed for multi-step reasoning, iterative refinement, and other advanced use cases.\\n* **Agents:**  LangChain provides \"agents\" which decide which actions to take, based on the LLM\\'s output. These actions can include things like searching the web, running calculations, or interacting with other APIs.  Agents dynamically determine the best course of action based on the current context.\\n* **Memory:**  Maintaining context across multiple interactions is crucial for many applications. LangChain offers various memory implementations to store and retrieve information from previous exchanges, allowing for more coherent and context-aware conversations.\\n* **Callbacks:**  Callbacks provide a mechanism to monitor and log the execution of chains and agents, offering insights into the inner workings of your application. This is invaluable for debugging, performance analysis, and understanding how the LLM is being used.\\n* **Indexes:**  LangChain offers tools for structuring documents specifically for LLMs. This can involve creating embeddings, splitting text into chunks, and storing them in a vector database for efficient retrieval.  Effective indexing significantly improves the performance and relevance of LLM responses, especially when working with large datasets.\\n\\n\\n**Key Features and Benefits:**\\n\\n* **Modularity and Composability:**  LangChain\\'s modular design allows you to easily swap out different components, experiment with various LLMs, and customize chains to fit your specific needs.\\n* **Simplified Prompt Management:**  LangChain provides tools for managing and optimizing prompts, making it easier to elicit desired responses from LLMs.\\n* **Integration with External Data Sources:**  Connect LLMs to APIs, databases, and other sources of information, enabling them to access and process real-world data.\\n* **Enhanced Contextual Awareness:**  Memory mechanisms allow for more coherent and contextually relevant interactions over multiple turns.\\n* **Improved Efficiency:**  Optimized data structures and algorithms ensure efficient processing and retrieval of information.\\n* **Extensibility:**  LangChain is designed to be extensible, allowing developers to add custom components and functionalities.\\n\\n\\n**Use Cases:**\\n\\n* **Chatbots:** Build conversational agents that can engage in natural language interactions.\\n* **Question Answering:** Develop systems that can answer questions based on specific documents or knowledge bases.\\n* **Text Summarization:**  Automatically generate concise summaries of longer texts.\\n* **Code Generation:** Use LLMs to generate code in various programming languages.\\n* **Data Analysis and Exploration:** Leverage LLMs to analyze and interpret data, identify patterns, and generate insights.\\n\\n\\n**Getting Started:**\\n\\nLangChain is available as a Python package.  You can install it using `pip install langchain`.  The documentation provides comprehensive guides, tutorials, and examples to help you get started.\\n\\n\\nIn summary, LangChain empowers developers to build sophisticated applications that harness the power of language models. Its modular design, rich feature set, and focus on integration make it a valuable tool for anyone working with LLMs.' additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []} id='run-93c373b8-539a-4031-adae-4f15180f0d82-0' usage_metadata={'input_tokens': 26, 'output_tokens': 821, 'total_tokens': 847, 'input_token_details': {'cache_read': 0}}\n"
     ]
    }
   ],
   "source": [
    "chain = prompt | llm\n",
    "\n",
    "# First interaction\n",
    "print(chain.invoke({\"input\": \"Tell me about LangChain\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangSmith is a platform designed specifically for debugging, testing, evaluating, and monitoring LLM applications.  It aims to streamline the iterative process of building and refining language model-driven systems. Here's a breakdown of its key features and benefits:\n",
      "\n",
      "**Key Features:**\n",
      "\n",
      "* **Tracing:** Captures a detailed history of your LLM interactions, including prompts, completions, chain of thought reasoning, metadata, and performance metrics.  This allows you to understand the flow of execution and pinpoint areas for improvement. Think of it as a comprehensive debugger for your LLM workflows.\n",
      "* **Debugging & Evaluation:** Provides tools for identifying and analyzing errors, inconsistencies, and biases in LLM outputs. This includes features for comparing different prompts and models, visualizing performance across various metrics, and generating test cases to ensure robustness.\n",
      "* **Monitoring & Analytics:** Tracks performance metrics over time, such as latency, cost, and completion quality. This helps you monitor the health of your LLM applications and identify potential issues before they impact users.\n",
      "* **Collaboration:** Facilitates collaboration among team members by providing a shared workspace for debugging, testing, and iterating on LLM workflows. This fosters knowledge sharing and speeds up the development process.\n",
      "* **Experiment Tracking:** Enables you to track different versions of your prompts, models, and other components of your LLM workflows. This allows you to compare performance across different experiments and identify the best-performing configurations.\n",
      "* **Integration with Popular LLM Frameworks:**  LangSmith seamlessly integrates with popular LLM frameworks like LangChain, LlamaIndex, and Hugging Face Transformers, making it easy to incorporate into existing workflows.  It also supports direct integrations with OpenAI, Anthropic, Cohere, and other model providers.\n",
      "* **Data Labeling and Feedback:**  While not its primary focus, LangSmith offers capabilities to collect feedback on model outputs and label data for improving model performance. This can be helpful for fine-tuning or reinforcement learning tasks.\n",
      "\n",
      "**Benefits of Using LangSmith:**\n",
      "\n",
      "* **Faster Iteration Cycles:** By providing a centralized platform for debugging and testing, LangSmith helps you identify and fix issues more quickly, accelerating the development process.\n",
      "* **Improved LLM Performance:** The detailed tracing and evaluation tools allow you to gain deep insights into the behavior of your LLMs, enabling you to optimize prompts, models, and workflows for better performance.\n",
      "* **Reduced Costs:** By identifying and eliminating inefficiencies in your LLM workflows, LangSmith can help you reduce the cost of running your applications.\n",
      "* **Enhanced Collaboration:** The collaborative features of LangSmith promote knowledge sharing and teamwork, leading to faster development and better outcomes.\n",
      "* **Greater Transparency and Explainability:** The detailed tracing and logging capabilities provide greater transparency into the inner workings of your LLM applications, making it easier to understand and explain their behavior.\n",
      "\n",
      "\n",
      "**Who Should Use LangSmith?**\n",
      "\n",
      "* **LLM Application Developers:** Anyone building applications powered by large language models can benefit from using LangSmith.\n",
      "* **Data Scientists and Machine Learning Engineers:**  Data scientists and ML engineers working with LLMs can use LangSmith to improve model performance and ensure the quality of their applications.\n",
      "* **AI Researchers:** Researchers can use LangSmith to study the behavior of LLMs and develop new techniques for improving their performance.\n",
      "\n",
      "In summary, LangSmith is a powerful platform that provides a comprehensive set of tools for building, debugging, testing, and monitoring LLM applications. Its focus on tracing, evaluation, and collaboration makes it an invaluable resource for anyone working with large language models.\n"
     ]
    }
   ],
   "source": [
    "# String Output Parser\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "output_parser=StrOutputParser()\n",
    "chain=prompt|llm|output_parser\n",
    "\n",
    "response=chain.invoke({\"input\":\"Can you tell me about Langsmith?\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'LangSmith', 'description': 'LangSmith is a platform designed to help developers and researchers close the gap between Large Language Model (LLM) experimentation and production-ready applications. It offers tools for debugging, testing, evaluating, and monitoring LLM workflows, enabling faster iteration and more reliable deployments.', 'key_features': [{'name': 'Debugging & Tracing', 'description': 'Allows detailed inspection of LLM executions to identify and resolve issues like hallucinations, unexpected outputs, or logic errors.  Provides insights into prompts, completions, chain of thought reasoning, and other relevant data.'}, {'name': 'Evaluation & Testing', 'description': 'Facilitates rigorous testing and evaluation of LLM performance through automated test generation, comprehensive metrics, and comparison across different models or prompt variations.'}, {'name': 'Monitoring & Analytics', 'description': 'Tracks key performance indicators (KPIs) like latency, cost, and error rates in real-time, enabling continuous monitoring of deployed LLM applications.  Helps identify trends and potential areas for optimization.'}, {'name': 'Version Control & Collaboration', 'description': 'Supports versioning of prompts, chains, and other components of LLM workflows, promoting reproducibility and collaboration among team members.'}, {'name': 'Data Management & Integration', 'description': 'Provides tools for managing training data, feedback data, and other datasets relevant to LLM development. Integrates with various data sources and LLM providers.'}], 'use_cases': ['Building chatbots and conversational agents', 'Developing AI-powered writing tools', 'Creating question-answering systems', 'Automating code generation and debugging', 'Improving search relevance and personalization', 'Generating synthetic data for training other models'], 'benefits': ['Faster iteration cycles for LLM development', 'Improved reliability and robustness of LLM applications', 'Reduced time and effort spent on debugging and testing', 'Enhanced collaboration and knowledge sharing among developers', 'Better understanding of LLM behavior and performance'], 'pricing': 'Offers various pricing plans, including a free tier for experimentation and paid plans for larger-scale deployments.', 'website': 'https://langsmith.com'}\n"
     ]
    }
   ],
   "source": [
    "# JSON Output Parser\n",
    "\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "output_parser=JsonOutputParser()\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query.\\n{format_instructions}\\n{input}\\n\",\n",
    "    input_variables=[\"input\"],\n",
    "    partial_variables={\"format_instructions\": output_parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "chain=prompt|llm|output_parser\n",
    "response=chain.invoke({\"input\":\"Can you tell me about Langsmith?\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.document_loaders.web_base.WebBaseLoader at 0x21e5243a750>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Building RAG with Web Page\n",
    "\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "loader = WebBaseLoader(\"https://docs.smith.langchain.com/evaluation/tutorials/evaluation\")\n",
    "loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/evaluation', 'title': 'Evaluate a chatbot | 🦜️🛠️ LangSmith', 'description': 'In this guide we will set up evaluations for a chatbot.', 'language': 'en'}, page_content='\\n\\n\\n\\n\\nEvaluate a chatbot | 🦜️🛠️ LangSmith\\n\\n\\n\\n\\n\\n\\nSkip to main contentLearn the essentials of LangSmith in the new Introduction to LangSmith course!  Enroll for free. API ReferenceRESTPythonJS/TSSearchRegionUSEUGo to AppGet StartedObservabilityEvaluationQuick StartTutorialsEvaluate a chatbotEvaluate a RAG applicationRun backtests on a new version of an agentRunning SWE-bench with LangSmithEvaluate a complex agentTest a ReAct agent with Pytest/Vitest and LangSmithHow-to GuidesCapture user feedback from your application to tracesHow to run an evaluationHow to manage datasets in the UIHow to bind an evaluator to a dataset in the UIHow to manage datasets programmaticallyHow to run an evaluation from the prompt playgroundSet up feedback criteriaAnnotate traces and runs inlineHow to evaluate an application\\'s intermediate stepsHow to version a datasetUse annotation queuesHow to share or unshare a dataset publiclyHow to use off-the-shelf evaluators (Python only)How to compare experiment resultsDynamic few shot example selectionHow to evaluate an existing experiment (Python only)How to export filtered traces from experiment to datasetHow to run evals with pytest (beta)Run pairwise evaluationsHow to audit evaluator scoresHow to create few-shot evaluatorsHow to fetch performance metrics for an experimentHow to use the REST APIHow to upload experiments run outside of LangSmith with the REST APIHow to run an evaluation asynchronouslyHow to define a custom evaluatorHow to evaluate on a split / filtered view of a datasetHow to evaluate on a specific dataset versionRun an evaluation with large file inputsHow to filter experiments in the UIHow to evaluate a langchain runnableHow to evaluate a langgraph graphHow to define an LLM-as-a-judge evaluatorHow to run an evaluation locally (beta, Python only)How to return categorical vs numerical metricsHow to return multiple scores in one evaluatorHow to handle model rate limitsHow to evaluate with repetitionsHow to define a summary evaluatorHow to run evals with Vitest/Jest (beta)Conceptual GuidePrompt EngineeringDeployment (LangGraph Platform)AdministrationSelf-hostingPricingReferenceCloud architecture and scalabilityAuthz and AuthnAuthentication methodsdata_formatsEvaluationDataset transformationsRegions FAQsdk_referenceEvaluationTutorialsEvaluate a chatbotOn this pageEvaluate a chatbot\\nIn this guide we will set up evaluations for a chatbot.\\nThese allow you to measure how well your application is performing over a fixed set of data.\\nBeing able to get this insight quickly and reliably will allow you to iterate with confidence.\\nAt a high level, in this tutorial we will:\\n\\nCreate an initial golden dataset to measure performance\\nDefine metrics to use to measure performance\\nRun evaluations on a few different prompts or models\\nCompare results manually\\nTrack results over time\\nSet up automated testing to run in CI/CD\\n\\nFor more information on the evaluation workflows LangSmith supports, check out the how-to guides, or see the reference docs for evaluate and its asynchronous aevaluate counterpart.\\nLots to cover, let\\'s dive in!\\nSetup\\u200b\\nFirst install the required dependencies for this tutorial.\\nWe happen to use OpenAI, but LangSmith can be used with any model:\\npip install -U langsmith openai\\nAnd set environment variables to enable LangSmith tracing:\\nexport LANGSMITH_TRACING=\"true\"export LANGSMITH_API_KEY=\"<Your LangSmith API key>\"export OPENAI_API_KEY=\"<Your OpenAI API key>\"\\nCreate a dataset\\u200b\\nThe first step when getting ready to test and evaluate your application is to define the datapoints you want to evaluate.\\nThere are a few aspects to consider here:\\n\\nWhat should the schema of each datapoint be?\\nHow many datapoints should I gather?\\nHow should I gather those datapoints?\\n\\nSchema: Each datapoint should consist of, at the very least, the inputs to the application.\\nIf you are able, it is also very helpful to define the expected outputs - these represent what you would expect a properly functioning application to output.\\nOften times you cannot define the perfect output - that\\'s okay! Evaluation is an iterative process.\\nSometimes you may also want to define more information for each example - like the expected documents to fetch in RAG, or the expected steps to take as an agent.\\nLangSmith datasets are very flexible and allow you to define arbitrary schemas.\\nHow many: There\\'s no hard and fast rule for how many you should gather.\\nThe main thing is to make sure you have proper coverage of edge cases you may want to guard against.\\nEven 10-50 examples can provide a lot of value!\\nDon\\'t worry about getting a large number to start - you can (and should) always add over time!\\nHow to get: This is maybe the trickiest part.\\nOnce you know you want to gather a dataset... how do you actually go about it?\\nFor most teams that are starting a new project, we generally see them start by collecting the first 10-20 datapoints by hand.\\nAfter starting with these datapoints, these datasets are generally living constructs and grow over time.\\nThey generally grow after seeing how real users will use your application, seeing the pain points that exist, and then moving a few of those datapoints into this set.\\nThere are also methods like synthetically generating data that can be used to augment your dataset.\\nTo start, we recommend not worrying about those and just hand labeling ~10-20 examples.\\nOnce you\\'ve got your dataset, there are a few different ways to upload them to LangSmith.\\nFor this tutorial, we will use the client, but you can also upload via the UI (or even create them in the UI).\\nFor this tutorial, we will create 5 datapoints to evaluate on.\\nWe will be evaluating a question-answering application.\\nThe input will be a question, and the output will be an answer.\\nSince this is a question-answering application, we can define the expected answer.\\nLet\\'s show how to create and upload this dataset to LangSmith!\\nfrom langsmith import Clientclient = Client()# Define dataset: these are your test casesdataset_name = \"QA Example Dataset\"dataset = client.create_dataset(dataset_name)client.create_examples(    inputs=[        {\"question\": \"What is LangChain?\"},        {\"question\": \"What is LangSmith?\"},        {\"question\": \"What is OpenAI?\"},        {\"question\": \"What is Google?\"},        {\"question\": \"What is Mistral?\"},    ],    outputs=[        {\"answer\": \"A framework for building LLM applications\"},        {\"answer\": \"A platform for observing and evaluating LLM applications\"},        {\"answer\": \"A company that creates Large Language Models\"},        {\"answer\": \"A technology company known for search\"},        {\"answer\": \"A company that creates Large Language Models\"},    ],    dataset_id=dataset.id,)\\nNow, if we go the LangSmith UI and look for QA Example Dataset in the Datasets & Testing page,\\nwhen we click into it we should see that we have five new examples.\\n\\nDefine metrics\\u200b\\nAfter creating our dataset, we can now define some metrics to evaluate our responses on.\\nSince we have an expected answer, we can compare to that as part of our evaluation.\\nHowever, we do not expect our application to output those exact answers, but rather something that is similar.\\nThis makes our evaluation a little trickier.\\nIn addition to evaluating correctness, let\\'s also make sure our answers are short and concise.\\nThis will be a little easier - we can define a simple Python function to measure the length of the response.\\nLet\\'s go ahead and define these two metrics.\\nFor the first, we will use an LLM to judge whether the output is correct (with respect to the expected output).\\nThis LLM-as-a-judge is relatively common for cases that are too complex to measure with a simple function.\\nWe can define our own prompt and LLM to use for evaluation here:\\nimport openaifrom langsmith import wrappersopenai_client = wrappers.wrap_openai(openai.OpenAI())eval_instructions = \"You are an expert professor specialized in grading students\\' answers to questions.\"def correctness(inputs: dict, outputs: dict, reference_outputs: dict) -> bool:    user_content = f\"\"\"You are grading the following question:{inputs[\\'question\\']}Here is the real answer:{reference_outputs[\\'answer\\']}You are grading the following predicted answer:{outputs[\\'response\\']}Respond with CORRECT or INCORRECT:Grade:\"\"\"    response = openai_client.chat.completions.create(        model=\"gpt-4o-mini\",        temperature=0,        messages=[            {\"role\": \"system\", \"content\": eval_instructions},            {\"role\": \"user\", \"content\": user_content},        ],    ).choices[0].message.content    return response == \"CORRECT\"\\nFor evaluating the length of the response, this is a lot easier!\\nWe can just define a simple function that checks whether the actual output is less than 2x the length of the expected result.\\ndef concision(outputs: dict, reference_outputs: dict) -> bool:    return int(len(outputs[\"response\"]) < 2 * len(reference_outputs[\"answer\"]))\\nRun Evaluations\\u200b\\nGreat! So now how do we run evaluations?\\nNow that we have a dataset and evaluators, all that we need is our application!\\nWe will build a simple application that just has a system message with instructions on how to respond and then passes it to the LLM.\\nWe will build this using the OpenAI SDK directly:\\ndefault_instructions = \"Respond to the users question in a short, concise manner (one short sentence).\"def my_app(question: str, model: str = \"gpt-4o-mini\", instructions: str = default_instructions) -> str:    return openai_client.chat.completions.create(        model=model,        temperature=0,        messages=[            {\"role\": \"system\", \"content\": instructions},            {\"role\": \"user\", \"content\": question},        ],    ).choices[0].message.content\\nBefore running this through LangSmith evaluations, we need to define a simple wrapper that maps the input keys from our dataset to the function we want to call,\\nand then also maps the output of the function to the output key we expect.\\ndef ls_target(inputs: str) -> dict:    return {\"response\": my_app(inputs[\"question\"])}\\nGreat!\\nNow we\\'re ready to run an evaluation.\\nLet\\'s do it!\\nexperiment_results = client.evaluate(    ls_target, # Your AI system    data=dataset_name, # The data to predict and grade over    evaluators=[concision, correctness], # The evaluators to score the results    experiment_prefix=\"openai-4o-mini\", # A prefix for your experiment names to easily identify them)\\nThis will output a URL. If we click on it, we should see results of our evaluation!\\n\\nIf we go back to the dataset page and select the Experiments tab, we can now see a summary of our one run!\\n\\nLet\\'s now try it out with a different model! Let\\'s try gpt-4-turbo\\ndef ls_target_v2(inputs: str) -> dict:    return {\"response\": my_app(inputs[\"question\"], model=\"gpt-4-turbo\")}experiment_results = client.evaluate(    ls_target_v2,    data=dataset_name,    evaluators=[concision, correctness],    experiment_prefix=\"openai-4-turbo\",)\\nAnd now let\\'s use GPT-4 but also update the prompt to be a bit more strict in requiring the answer to be short.\\ninstructions_v3 = \"Respond to the users question in a short, concise manner (one short sentence). Do NOT use more than ten words.\"def ls_target_v3(inputs: str) -> dict:    response = my_app(        inputs[\"question\"],         model=\"gpt-4-turbo\",        instructions=instructions_v3    )    return {\"response\": response}experiment_results = client.evaluate(    ls_target_v3,    data=dataset_name,    evaluators=[concision, correctness],    experiment_prefix=\"strict-openai-4-turbo\",)\\nIf we go back to the Experiments tab on the datasets page, we should see that all three runs now show up!\\n\\nComparing results\\u200b\\nAwesome, we\\'ve evaluated three different runs. But how can we compare results?\\nThe first way we can do this is just by looking at the runs in the Experiments tab.\\nIf we do that, we can see a high level view of the metrics for each run:\\n\\nGreat! So we can tell that GPT-4 is better than GPT-3.5 at knowing who companies are, and we can see that the strict prompt helped a lot with the length.\\nBut what if we want to explore in more detail?\\nIn order to do that, we can select all the runs we want to compare (in this case all three) and open them up in a comparison view.\\nWe immediately see all three tests side by side.\\nSome of the cells are color coded - this is showing a regression of a certain metric compared to a certain baseline.\\nWe automatically choose defaults for the baseline and metric, but you can change those yourself.\\nYou can also choose which columns and which metrics you see by using the Display control.\\nYou can also automatically filter to only see the runs that have improvements/regressions by clicking on the icons at the top.\\n\\nIf we want to see more information, we can also select the Expand button that appears when hovering over a row to open up a side panel with more detailed information:\\n\\nSet up automated testing to run in CI/CD\\u200b\\nNow that we\\'ve run this in a one-off manner, we can set it to run in an automated fashion.\\nWe can do this pretty easily by just including it as a pytest file that we run in CI/CD.\\nAs part of this, we can either just log the results OR set up some criteria to determine if it passes or not.\\nFor example, if I wanted to ensure that we always got at least 80% of generated responses passing the length check,\\nwe could set that up with a test like:\\ndef test_length_score() -> None:    \"\"\"Test that the length score is at least 80%.\"\"\"    experiment_results = evaluate(        ls_target, # Your AI system        data=dataset_name, # The data to predict and grade over        evaluators=[concision, correctness], # The evaluators to score the results    )    # This will be cleaned up in the next release:    feedback = client.list_feedback(        run_ids=[r.id for r in client.list_runs(project_name=experiment_results.experiment_name)],        feedback_key=\"concision\"    )    scores = [f.score for f in feedback]    assert sum(scores) / len(scores) >= 0.8, \"Aggregate score should be at least .8\"\\nTrack results over time\\u200b\\nNow that we\\'ve got these experiments running in an automated fashion, we want to track these results over time.\\nWe can do this from the overall Experiments tab in the datasets page.\\nBy default, we show evaluation metrics over time (highlighted in red).\\nWe also automatically track git metrics, to easily associate it with the branch of your code (highlighted in yellow).\\n\\nConclusion\\u200b\\nThat\\'s it for this tutorial!\\nWe\\'ve gone over how to create an initial test set, define some evaluation metrics, run experiments, compare them manually, set up CI/CD, and track results over time.\\nHopefully this can help you iterate with confidence.\\nThis is just the start. As mentioned earlier, evaluation is an ongoing process.\\nFor example - the datapoints you will want to evaluate on will likely continue to change over time.\\nThere are many types of evaluators you may wish to explore.\\nFor information on this, check out the how-to guides.\\nAdditionally, there are other ways to evaluate data besides in this \"offline\" manner (e.g. you can evaluate production data).\\nFor more information on online evaluation, check out this guide.\\nReference code\\u200b\\nClick to see a consolidated code snippetimport openaifrom langsmith import Client, wrappers# Application codeopenai_client = wrappers.wrap_openai(openai.OpenAI())default_instructions = \"Respond to the users question in a short, concise manner (one short sentence).\"def my_app(question: str, model: str = \"gpt-4o-mini\", instructions: str = default_instructions) -> str:    return openai_client.chat.completions.create(        model=model,        temperature=0,        messages=[            {\"role\": \"system\", \"content\": instructions},            {\"role\": \"user\", \"content\": question},        ],    ).choices[0].message.contentclient = Client()# Define dataset: these are your test casesdataset_name = \"QA Example Dataset\"dataset = client.create_dataset(dataset_name)client.create_examples(    inputs=[        {\"question\": \"What is LangChain?\"},        {\"question\": \"What is LangSmith?\"},        {\"question\": \"What is OpenAI?\"},        {\"question\": \"What is Google?\"},        {\"question\": \"What is Mistral?\"},    ],    outputs=[        {\"answer\": \"A framework for building LLM applications\"},        {\"answer\": \"A platform for observing and evaluating LLM applications\"},        {\"answer\": \"A company that creates Large Language Models\"},        {\"answer\": \"A technology company known for search\"},        {\"answer\": \"A company that creates Large Language Models\"},    ],    dataset_id=dataset.id,)# Define evaluatorseval_instructions = \"You are an expert professor specialized in grading students\\' answers to questions.\"def correctness(inputs: dict, outputs: dict, reference_outputs: dict) -> bool:    user_content = f\"\"\"You are grading the following question:{inputs[\\'question\\']}Here is the real answer:{reference_outputs[\\'answer\\']}You are grading the following predicted answer:{outputs[\\'response\\']}Respond with CORRECT or INCORRECT:Grade:\"\"\"    response = openai_client.chat.completions.create(        model=\"gpt-4o-mini\",        temperature=0,        messages=[            {\"role\": \"system\", \"content\": eval_instructions},            {\"role\": \"user\", \"content\": user_content},        ],    ).choices[0].message.content    return response == \"CORRECT\"def concision(outputs: dict, reference_outputs: dict) -> bool:    return int(len(outputs[\"response\"]) < 2 * len(reference_outputs[\"answer\"]))# Run evaluationsdef ls_target(inputs: str) -> dict:    return {\"response\": my_app(inputs[\"question\"])}experiment_results_v1 = client.evaluate(    ls_target, # Your AI system    data=dataset_name, # The data to predict and grade over    evaluators=[concision, correctness], # The evaluators to score the results    experiment_prefix=\"openai-4o-mini\", # A prefix for your experiment names to easily identify them)def ls_target_v2(inputs: str) -> dict:    return {\"response\": my_app(inputs[\"question\"], model=\"gpt-4-turbo\")}experiment_results_v2 = client.evaluate(    ls_target_v2,    data=dataset_name,    evaluators=[concision, correctness],    experiment_prefix=\"openai-4-turbo\",)instructions_v3 = \"Respond to the users question in a short, concise manner (one short sentence). Do NOT use more than ten words.\"def ls_target_v3(inputs: str) -> dict:    response = my_app(        inputs[\"question\"],         model=\"gpt-4-turbo\",        instructions=instructions_v3    )    return {\"response\": response}experiment_results_v3 = client.evaluate(    ls_target_v3,    data=dataset_name,    evaluators=[concision, correctness],    experiment_prefix=\"strict-openai-4-turbo\",)Was this page helpful?You can leave detailed feedback on GitHub.PreviousEvaluation tutorialsNextEvaluate a RAG applicationSetupCreate a datasetDefine metricsRun EvaluationsComparing resultsSet up automated testing to run in CI/CDTrack results over timeConclusionReference codeCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright © 2025 LangChain, Inc.\\n\\n')]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document = loader.load()\n",
    "document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "document = text_splitter.split_documents(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For OpenAI\n",
    "# from langchain_openai import OpenAIEmbeddings\n",
    "# embedings=OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GoogleGenerativeAIEmbeddings(client=<google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient object at 0x0000021E2CCC4200>, model='models/text-embedding-004', task_type=None, google_api_key=SecretStr('**********'), credentials=None, client_options=None, transport=None, request_options=None)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "embeddings= GoogleGenerativeAIEmbeddings(model=\"models/text-embedding-004\")\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x21e5243ff80>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "vector_store = FAISS.from_documents(document, embeddings)\n",
    "vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'We can define our own prompt and LLM to use for evaluation here:\\nimport openaifrom langsmith import wrappersopenai_client = wrappers.wrap_openai(openai.OpenAI())eval_instructions = \"You are an expert professor specialized in grading students\\' answers to questions.\"def correctness(inputs: dict, outputs: dict, reference_outputs: dict) -> bool:    user_content = f\"\"\"You are grading the following question:{inputs[\\'question\\']}Here is the real answer:{reference_outputs[\\'answer\\']}You are grading the following predicted answer:{outputs[\\'response\\']}Respond with CORRECT or INCORRECT:Grade:\"\"\"    response = openai_client.chat.completions.create(        model=\"gpt-4o-mini\",        temperature=0,        messages=[            {\"role\": \"system\", \"content\": eval_instructions},            {\"role\": \"user\", \"content\": user_content},        ],    ).choices[0].message.content    return response == \"CORRECT\"\\nFor evaluating the length of the response, this is a lot easier!'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"For evaluating the length of the response\"\n",
    "\n",
    "result = vector_store.similarity_search(query)\n",
    "result[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
